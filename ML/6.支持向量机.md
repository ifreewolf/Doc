# <b><font color=red size=6px>一.间隔与支持向量</font></b>
给定样本集$D=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\},y_i\in\{-1,+1\}$，分类学习器基于数据集$D$可以找到如下图(1)所示的多个划分超平面，图中位于两类训练样本"正中间"的粗线，是我们努力寻找的最优划分超平面。
<table align=center>
    <tr><td align=center><img src="./images/支持向量机1.png"></td></tr>
    <tr><td align=center><b><font size=4px>图(1) 存在多个划分超平面将两类训练样本分开</font></b></td></tr>
</table>

<b><font color=-red size=4px>最优超平面的特点</font></b>：对训练样本局部扰动的“容忍”性最好。

在样本空间中，划分超平面可通过如下线性方程来描述：
$$w^Tx+b=0, \tag{1}$$

>$w=(w_1;w_2;...;w_d)$为法向量，决定了超平面的方向；$b$为位移项，决定了超平面与原点之间的距离。<font color=red size=3px>因此超平面由法向量$w$和位移$b$确定,记为$(w,b)$</font>。

样本空间中任意点$x$到超平面$(w,b)$的距离可写成：
$$r=\frac{|w^Tx+b|}{\lVert w\rVert}. \tag{2}$$

若超平面$(w,b)$可以将训练样本正确分类，则对于$(x_i,y_i)\in D$，若$y_i=+1$，则有$w^Tx_i+b>0$；若$y_i=-1$，则有$w^Tx_i+b<0$，令
$$
\begin{cases}
    w^Tx_i+b\ge+1,  & y_i=+1; \\
    w^Tx_i+b\le-1, & y_i=-1.
\end{cases} \tag{3}
$$

<table align=center>
    <tr><td align=center><img src="./images/支持向量机2.png"></td></tr>
    <tr><td align=center><b><font size=4px>图(2) 支持向量与间隔</font></b></td></tr>
</table>
如上图(2)所示，距离超平面最近的几个样本点可以使上式(3)成立，这几个样本也被称为支持向量，两个不同类支持向量到超平面的距离之和为:

$$\gamma=\frac{2}{\lVert w\rVert}, \tag{4}$$
>它被称为"间隔"。<br />
>向量$u$在向量$v$上的投影$d=\overrightarrow{u}cos\theta=\frac{\overrightarrow{u}\overrightarrow{v}}{\lVert v\rVert}$; <br />

---
---
假设 $x'$，$x''$在超平面上
根据式(1)可得：<br />
①$w^Tx'=-b,w^Tx''=-b$<br />
②w$\bot$hyperplane<br />
$(w^T(x''-x'))=0$<br />
$\left(w^T\begin{matrix} \underbrace{ (x''-x') } \\ vector on huperplane\end{matrix}\right)=0$<br />
③distance=project$(x-x')$to$\bot$hyperplane<br />
$distance(x,b,w)=\left\lvert\frac{w^T}{\lVert w \rVert}(x-x')\right\rvert=\frac{1}{\lVert w \rVert}(w^Tx-w^Tx')=\frac{1}{\lVert w \rVert}(w^Tx+b)$
>因此，对于支持向量，$w^Tx+b=1$, 所以，式子(4)可以根据上式推算出来。添加2只是为了后面公式(6)求导可以消去一个参数。

---
---

要找<font color=red size=3px>最大间隔</font>的划分超平面，也就是要找到能满足式(3)中约束的参数$w$和$b$，使得$\gamma$最大，即
$$
\begin{aligned}
    &max_{w,b}\frac{2}{\lVert w\rVert} \\
    &s.t. y_i(w^Tx_i+b)\ge1,i=1,2,...,m
\end{aligned} \tag{5}
$$
>显然，最大化间隔，仅需最大化$\lVert w\rVert^{-1}$，等价于最小化$\lVert w\rVert^2$。于是，式(5)可重写为
$$
\begin{aligned}
    &min_{w,b}\frac{1}{2}{\lVert w\rVert}^2 \\
    &s.t. y_i(w^Tx_i+b)\ge1,i=1,2,...,m
\end{aligned} \tag{6}
$$
>以上是支持向量机的基本型。


# <b><font color=red size=6px>二.对偶问题</font></b>
求解式(6)获得大间隔划分超平面的模型如下：
$$f(x)=w^Tx+b, \tag{7}$$
>式(6)使用拉格朗日乘子法可得到“对偶问题”，拉格朗日乘子法是对每个约束条件添加拉格朗日乘子$\alpha_i\ge 0$,得到如下拉格朗日函数：
$$L(w,b,\alpha)=\frac{1}{2}\lVert w \rVert^2+\sum_{i=1}^{m}\alpha_i(1-y_i(w^Tx_i+b)), \tag{8}$$
>$L(w,b,\alpha)$对$w$和$b$求偏导：

___
___
$$
\begin{aligned}
    L(w,b,\alpha)&=\frac{1}{2}\lVert w \rVert^2+\sum_{i=1}^{m}\alpha_i(1-y_i(w^Tx_i+b)) \\
    &=\frac{1}{2}\lVert w \rVert^2 + \sum_{i=1}^{m}(\alpha_i - \alpha_iy_iw^Tx_i - \alpha_iy_ib) \\
    &=\frac{1}{2}\lVert w \rVert^2 + \sum_{i=1}^{m}\alpha_i - \sum_{i=1}^{m}\alpha_iy_iw^Tx_i - \sum_{i=1}^{m}\alpha_iy_ib
\end{aligned}
$$

$$
    \frac{\partial L}{\partial w} =\frac{1}{2}\times 2 \times w + 0 - \sum_{i=1}^{m}\alpha_iy_ix_i - 0 = 0\Rightarrow w=\sum_{i=1}^{m}\alpha_iy_ix_i
$$

$$
    \frac{\partial L}{\partial b} = 0 + 0 - 0 - \sum_{i=1}^{m}\alpha_iy_i = 0 \Rightarrow \sum_{i=1}^{m}\alpha_iy_i=0
$$
___
___
得到如下：
$$w=\sum_{i=1}^{m}\alpha_iy_ix_i, \tag{9}$$
$$\sum_{i=1}^{m}\alpha_iy_i=0, \tag{10}$$

将式(9)和式(10)代入式(8)，可得：
$$
\begin{aligned}
    L(w,b,\alpha) &= \frac{1}{2}\lVert w \rVert^2 + \sum_{i=1}^{m}\alpha_i(1-y_i(w^Tx_i+b)) \\
    &=\frac{1}{2} w^Tw+\sum_{i=1}^{m}\alpha_i - \sum_{i=1}^{m}\alpha_iy_iw^Tx_i-\sum_{i=1}^{m}\alpha_iy_ib \\
    &=\frac{1}{2}w^T\sum_{i=1}^{m}\alpha_iy_ix_i + \sum_{i=1}^{m}\alpha_i - w^T\sum_{i=1}^{m}\alpha_iy_ix_i - b\sum_{i=1}^{m}\alpha_iy_i \\
    &=-\frac{1}{2}w^T\sum_{i=1}^{m}\alpha_iy_ix_i + \sum_{i=1}^{m}\alpha_i \\
    &=-\frac{1}{2}(\sum_{i=1}^{m}\alpha_iy_ix_i)^T\sum_{i=1}^{m}\alpha_iy_ix_i + \sum_{i=1}^{m}\alpha_i \\
    &=-\frac{1}{2}\sum_{i=1}^{m}\alpha_iy_ix_i^T\sum_{i=1}^{m}\alpha_iy_ix_i + \sum_{i=1}^{m}\alpha_i \\
    &=\sum_{i=1}^{m}\alpha_i - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m} \alpha_i\alpha_jy_iy_jx_i^Tx_j \\
\end{aligned} \\ 
\longrightarrow \\
$$

$$
\begin{aligned}
    \max_{\alpha} \sum_{i=1}^{m}\alpha_i - &\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m} \alpha_i\alpha_jy_iy_jx_i^Tx_j \\
    s.t. &\sum_{i=1}^{m}\alpha_iy_i=0, \\
    &\alpha_i\ge0, i=1,2,...,m.  
\end{aligned} \tag{11} 
$$
解出$\alpha$后，求出$w$和$b$即可得到模型
$$
\begin{aligned}
    f(x) &= w^Tx+b \\
    &=\sum_{i=1}^{m}\alpha_iy_ix_i^Tx+b
\end{aligned} \tag{12}
$$



# <b><font color=red size=6px>三.核函数</font></b>
# <b><font color=red size=6px>四.软间隔与正则化</font></b>
# <b><font color=red size=6px>五.支持向量回归</font></b>
# <b><font color=red size=6px>六.核方法</font></b>
# <b><font color=red size=6px>七.软间隔与正则化</font></b>



<b><font color=-red size=4px></font></b>
<b><font color=-red size=4px></font></b>
<b><font color=-red size=4px></font></b>
<b><font color=-red size=4px></font></b>
<b><font color=-red size=4px></font></b>


<table align=center>
    <tr><td align=center><img src="./images/剪枝处理2.png"></td></tr>
    <tr><td align=center><b><font size=4px>图(5) 基于表4.2生成的未剪枝决策树</font></b></td></tr>
</table>