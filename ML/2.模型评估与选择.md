# <div align=center><b><font>模型评估与选择</font></b></div>

## <b><font size=5px>一.经验误差与过拟合</font></b>

<b><font color=red size=3px>错误率(error rate)</font></b>：分类错误的样本数 占 样本总数的比例，比如m个样本中，有a个样本分类错误，则错误率$E=a/m$;

<b><font color=red size=3px>精度(accuracy)</font></b>：$1-a/m$即为精度，表示分类正确的样本数 占 样本总数的比例；

<b><font color=red size=3px>误差(error)</font></b>：学习器的实际输出与样本的真实输出之间的差异称为“误差”。在训练集上的误差被称为“训练误差”或“经验误差”；在新样本上的误差被称为“泛化误差”。

>有必要解释一下误差和错误率：概念比较相似，错误率是针对整个样本来说的，统计分类错误的样本数比率，直接反应学习器的性能；误差是针对单个样本的输出与真实值之间的差异，通常在回归任务中比较重要。

>我们希望得到的学习器在新样本上表现得更好，也就是泛化误差尽可能小，但是新样本我们没有标签，无法知道新样本的泛化误差，因此能做的只有让训练误差最小化。

<b><font color=red size=3px>过拟合、欠拟合</font></b>：学习器从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”，这样才能在遇到新样本时做出正确的判别。但是，学习器把训练集学的太好，很可能已经把训练样本本身的一些特点当成了所有潜在样本的特征。这样即使训练误差非常小，在训练集上分类精度为100%，但在新样本上也有可能出现非常低的分类精度。这种现象被称为“过拟合”(overfitting)，与过拟合相对的是"欠拟合"(underfitting)，这是指对训练样本学习的不够好。

>过拟合产生的原因：学习能力过于强大，把训练集中的一般特征当成了所有样本的特征；欠拟合产生的原因：学习不到位，多增加训练的轮数。过拟合无法避免，只能缓解。

## <b><font size=5px>二.评估方法</font></b>
<b><font color=red size=3px>背景</font></b>：在训练学习器时，不同的参数选择可以得到不同的模型，理想情况下，希望通过模型的泛化误差来选择最优的模型；但是，我们没有办法获得模型的泛化误差(不知道新样本的标签),训练误差也无法作为模型的泛化衡量标准，因为存在过拟合。<br />
在实际中，通过实验测试来检验算法模型的泛化误差是比较常用的方法；因此，我们需要一个“测试集”来测试学习器对新样本的判别能力，然后用测试集上的“测试误差”来近似“泛化误差”。<br />

>用测试误差来近似泛化误差的前提是：保证测试集是从真实样分布中独立同分布采样，且尽可能与训练集互斥。

对于包含m个样例的数据集$D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$，既要训练，又要测试？需要通过适当的处理，从中产生训练集S和测试集T,下面介绍分离训练集和测试集的方法。

### <b><font size=4px>2.1 留出法</font></b>
“留出法”将数据集D划分为两个互斥的集合，其中一个作为训练集S，另一个作为测试集T,即$D=S\cup T,S\cap T=\emptyset$；用S来训练模型，用T来测试训练好的模型。

<b><font color=red size=3px>分层采样</font></b>：训练集/测试集的划分尽可能保证数据的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响。比如在分类任务中，不能简单将数据随机分成两份，而是应该根据类别进行同比例划分。比如：假设数据集有5个类别，希望将所有数据按照7：3划分为训练集和测试集，划分规则应该是每个类别都按照7:3去划分数据，而不是所有数据随机划分为7:3。这种保留类别比例的采样方式通常称为“分层采样”.

>比如对于样本集D,二分类包含500个正例，500个反例，按照7：3的比例划分；则训练集S中应该包含350个正例、350个反例，测试集包含150个正例、150个反例。若S、T的样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异产生偏差。比如：训练集S包含所有正例，以及200个反例；则训练得到的模型将有大概率将负样本预测为正样本。

<b><font color=red size=3px>问题1</font></b>：即使使用分层采样，依然存在多种划分方式，不同的划分(选取的样本)将导致不同的训练、测试集，相应的，模型评估的结果也会有茶杯。单次使用留出法得到的评估结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行试验评估后取平均值作为留出法的评估结果。

<b><font color=red size=3px>问题2</font></b>：如果数据集D样本比较少，我们希望评估的是用D训练出的模型的性能，但留出法需划分训练集/测试集。训练集S越大，训练出的模型可能更接近于用D训练出的模型，但由于T比较小，评估的结果可能不够稳定准确；若T包含较多的样本，则训练集S与D差别更大，被评估的模型与D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性，这个问题至今没有很好的解决方案。但是对于数据量比较大的数据集，则影响越小。通常将整个数据集的2/3~4/5的样本用于训练，剩余样本用于测试。

### <b><font size=4px>2.2 交叉验证法</font></b>
“交叉验证法”先将数据集D划分为k个大小相似的互斥子集，即$D=D_1\cup D_2\cup ... \cup D_k, D_i \cap D_j = \emptyset(i \ne j)$. 每个子集$D_i$都尽可能保持数据分布的一致性，即从D中分层采样获得。每次用k-1个子集的并集作为训练集，余下的子集作为测试集；这样就可以获得k组训练集/测试集，从而可以得到k次训练和测试，最终返回k个测试结果的均值。

>交叉验证法评估结果的稳定性和保真性很大程度取决于k的取值，因此交叉验证法又叫“k折交叉验证”(k-fold cross validation)。k常取10，因此称为10折交叉验证。<br />
对于每次的k个子集的划分，通常需要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值；换句话说，k折交叉验证法会把数据集划分为k个子集，训练k次，每次的划分又会重复p次，因此k折交叉验证法的训练次数是kxp。比如10次10折交叉验证的训练次数是100次；100次留出法的训练次数也是100次。

<b><font color=red size=3px>留一法</font></b>：若k=m，则交叉验证法就变成了留一法。留一法不受随机样本划分的影响，因为m个样本只有唯一的方式划分为m个子集-每个子集包含一个样本；留一法使用的训练集和初始数据集相差一个样本，因此，留一法评估的模型与用D评估的模型很相似，结果比较准确。

>留一法的缺陷：数据集较大时，训练m个模型的计算开销无法忍受；另外，留一法的估计结果也未必永远比其他评估方法准确--“没有免费午餐定理”也适用于实验评估方法。

### <b><font size=4px>2.3 自助法</font></b>
留出法和交叉验证法都要求一部分数据集用于测试，因此实际训练样本的规模与使用的数据集存在差异，会引入因样本规模不同而导致的估计偏差。留一法受训练样本的规模变化的影响较小，但计算复杂度太高。

“自助法”可以比较好的解决数据规模差异造成的影响而且可以比较高效的进行实验：对于数据集D，每次从中有放回的采样一个样本，将其拷贝放入$D'$集合中，重复m次，则可以得到一个包含m个样本的集合D'。D中的部分样本会在D'中多次出现，肯定也存在一些样本不会出现，也就是说D'中包含一些重复的样本。

>使用数学公式估计一下未出现在集合D'中的数量,样本m次采样中始终不被采到的概率是$(1-\frac{1}{m})^m$，取极限得到：
$$
\lim_{a\to \infty} (1-\frac{1}{m})^m=\frac{1}{e}\approx0.368, \tag{1}
$$
>可知，初始数据集D中约有36.8%的样本未出现在集合D'中。因此可以比较科学地将D'当成训练集，D\D'用作测试集，也就是剩下的36.8%。

<b><font color=red size=3px>总结</font></b>：自助法中训练集和原始数据集数量相同，而测试集还有约1/3的，没出现在训练集的数据。这样的测试结果亦称为<font color=red>“包外估计”</font>。自助法适用于数据集较小、难以有效划分训练集/测试集的情况。且自助法中，可以从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。

><font color=red>缺点</font>：自助法产生的数据集改变了初始数据集的分布，会引入估计偏差。因此，在初始数据量足够时，留出法和交叉验证法更常用。

### <b><font size=4px>2.4 调参与最终模型</font></b>
大多数机器学习算法都有些参数需要设定，参数配置不同，获得的模型性能存在显著的差别。因此，在进行模型评估与选择时，除了要对合适的学习算法进行选择外，还需要对算法参数进行设定，这个过程称为“参数调节”或“调参”.

理想状态下，参数的选择可以像算法选择一样，对每种参数配置都训练出模型，然后筛选出最好的模型参数；但在实际中，参数的范围是整个实数域，对每个参数都训练出模型算力将无法忍受。因此，常用的做法是对参数选定一个范围[0,0.2]，然后以变化的步长去训练模型，比如步长为0.05，则有5个参数需要训练。显然，这样选定的参数值或许不是“最佳”值，但这是计算开销和性能评估之间进行折中的结果。

参数的调节对于最终模型性能有关键性的影响，因此从事于机器学习的工程师又被称为调参工程师。

即使选择固定范围和步长，但一个模型有多个参数，计算量也还是比较大的，比如某个模型有3个参数，每个参数都考虑5个候选值，这样也会有$5^3=125个模型需要考察$。

<b><font color=red size=3px>最终模型</font></b>：模型完成训练后，学习算法和参数配置已选定，此时应该用数据集D所有数据重新训练模型，得到最终的模型。

## <b><font size=5px>三.性能度量</font></b>
性能度量就是对学习器泛化性能的评估标准。对于不同的模型，使用不同的性能度量会导致不通的评判结果；意味着模型的好坏是相对的，模型的好坏不仅取决于算法和数据，还决定于任务需求。

<b><font color=red size=3px>均方误差</font></b>：均方误差是回归任务中最常用的性能度量,(Mean Squared Error,MSE)
$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2. \tag{2}
$$

对于数据分布D和概率密度函数$p(\cdot)$，均方误差可描述为：
$$
E(f;D)=\int_{x~D}(f(x)-y)^2p(x)dx. \tag{3}
$$
### <b><font size=4px>3.1 错误率与精度</font></b>
错误率和精度是分类任务中最常用的两种性能度量；

<b><font color=red size=3px>错误率</font></b>：分类错误的样本数 占 样本总数的比例；
$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}\mathbb{I}(f(x_i)\ne y_i). \tag{4}
$$
<b><font color=red size=3px>精度</font></b>：分类正确的样本数占样本总数的比例
$$
\begin{aligned}
acc(f;D)&=\frac{1}{m}\sum_{i=1}^{m}\mathbb{I}(f(x_i) = y_i) \\
&=1-E(f;D). \tag{5}
\end{aligned}
$$

更一般地，对于数据分布D和概率密度函数$p(\cdot)$，错误率与精度可分别描述为：
$$
E(f;D)=\int_{x~D}\mathbb{I}(f(x_i)\ne y_i)p(x)dx. \tag{6}
$$

$$
\begin{aligned}
acc(f;D)&=\int_{x~D}\mathbb{I}(f(x_i) = y_i)p(x)dx \\
&=1-E(f;D). \tag{5}
\end{aligned}
$$

### <b><font size=4px>3.2 查准率、查全率与F1</font></b>
错误率和精度是常用的指标，但并不能满足所有任务需求。比如对于西瓜的评判，错误率可以衡量多少比例的瓜被判别为错误；有时候还会考虑挑出的好瓜中，有多少个是真正的好瓜，这个是查准率；或者说所有好瓜中，有多少比例被挑出来了，这个是查全率。

<div align=center><font size=4px color=red>混淆矩阵</font></div>
<table align=center border="2" cellspacing="8">
<tr><td rowspan="2">真实情况</td><td colspan="2" align=center>预测结果</td></tr>
<tr><td align=center>正例</td><td>反例</td></tr>
<tr><td align=center>正例</td><td>TP(真正例)</td><td>FN(假反例)</td></tr>
<tr><td align=center>反例</td><td>FP(假正例)</td><td>TN(真反例)</td></tr>
</table>
<b><font color=red size=3px>真正例(TP)</font></b>：正例被算法预测为正例<br />
<b><font color=red size=3px>假正例(FP)</font></b>：反例被算法预测为正例<br />
<b><font color=red size=3px>真反例(FN)</font></b>：正例被算法预测为反例<br />
<b><font color=red size=3px>假反例(TN)</font></b>：反例被算法预测为反例<br />

>有TP+FP+TN+FN=样例总数。

<b><font color=red size=3px>查准率P</font></b>：$P=\frac{TP}{TP+FP}$

<b><font color=red size=3px>查全率R</font></b>：$R=\frac{TP}{TP+FN}$

><font size=4px color=red>注</font>：查准率和查全率是一对矛盾的度量：查全率越高，意味着会将更多的样本预测为正，但这样查准率必然下降，会有更多的样本预测错误；若希望查准率更高，就需要挑选最有把握的瓜，但这样会漏掉更多的好瓜，使得查全率下降。在一些简单任务中，才可能使查全率和查准率都很高。

在实际应用中，通常根据学习器的预测结果(置信度)对预测样本进行排序，排在前面的是学习器认为最有可能是正例的样本，排在后面的则是学习器认为最不可能是正例的样本，按照这个顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率。解释一下：就是排序后，可以将样本分为正样本和负样本，然后还有测试样本的实际值，就可以得到混淆矩阵，自然可以预测查全率和查准率。

<b><font color=red size=3px>P-R曲线</font></b>：以查准率为纵轴、查全率为横轴，得到一个曲线：
<div align=center>
    <img src="./images/性能度量1.png" size=50%>
</div>
若一个学习器的P-R曲线被另一个学习器的P-R曲线完全包裹，则认为后者学习器的性能优于前者。上图的学习器就优于学习器C，如果两个学习器的P-R曲线有交叉，则难以判断哪个学习器好，但总需要一个优劣，因此通过比较P-R曲线下的面积大小，也可以在一定程度上表征学习器的查准率和查全率取得相对“双高”的比例。但是这个指标不太容易估算，因此又设计了一个综合考虑查准率和查全率的性能度量。<br />

<b><font color=red size=3px>平衡点(Break-Even Point,BEP)</font></b>：平衡点是查准率=查全率 时的取值，上图中学习器C的BEP是0.64，基于BEP可以认为学习器A优于学习器B.

<b><font color=red size=3px>F1</font></b>：BEP指标过于简化，F1是较为常用的度量指标：
$$
F1=\frac{2\times P\times R}{P+R}=\frac{2\times TP}{样例总数+TP-TN}. \tag{10}
$$

<b><font color=red size=3px>$F_{\beta}$</font></b>：F1对于查准率和查全率的重视程度一样，但是在实际应用中，对于不同的任务，对查准率和查全率的重视程度是不一样的。设计了一个F1的一般形式--$F_{\beta}$，定义如下：
$$
F_{\beta}=\frac{(1+\beta^2)\times P\times R}{(\beta^2\times P)+R}
$$

>$\beta$>0度量了查全率对查准率的相对重要性，当$\beta$=1时退化为标准的F1.$\beta$>1时，查全率有更大影响；$\beta$<1时查准率有更大的影响。

对于前面的留出法、交叉验证法等，会对同一个学习器进行多次训练和测试，每次都可以得到一个混淆矩阵；在多个数据集上进行训练/测试，希望估计算法的“全局”性能；执行多分类任务时，每两两类别的组合都对应一个矩阵；总之，需要在n个二分类混淆矩阵上综合考察查准率和查全率。

最直接的做法就是在各混淆矩阵上分别计算出查准率和查全率，记为$(P_1,R_1),(P_2,R_2),...,(P_n,R_n)$,再计算平均值，这样就得到“宏查准率”(macro-P)、“宏查全率”(macro-R)，以及相应的“宏F1”(macro-F1):
$$
macro-P=\frac{1}{n}\sum_{1}^{n}P_i, \tag{12}
$$

$$
macro-R=\frac{1}{n}\sum_{1}^{n}R_i, \tag{13}
$$

$$
macro-F1=\frac{2\times macro-P\times macro-R}{macro-P+macro-R}. \tag{14}
$$

“宏查准率”(macro-P)、“宏查全率”(macro-R)，以及相应的“宏F1”(macro-F1)定义如下：
$$
micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}, \tag{15}
$$

$$
micro-R=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}, \tag{16}
$$

$$
micro-P=\frac{2\times micro-P\times micro-R}{micro-P+micro-R}. \tag{17}
$$

### <b><font size=4px>3.3 ROC与AUC</font></b>
### <b><font size=4px>3.4 代价敏感错误率与代价曲线</font></b>





## <b><font size=5px>四.比较检验</font></b>
### <b><font size=4px>4.1 假设检验</font></b>
### <b><font size=4px>4.2 交叉验证t检验</font></b>
### <b><font size=4px>4.3 McNemar</font></b>




## <b><font size=5px>五.偏差与方差</font></b>

<b><font color=red size=3px></font></b>
<b><font color=red size=3px></font></b>
<b><font color=red size=3px></font></b>
<b><font color=red size=3px></font></b>
<b><font color=red size=3px></font></b>
<b><font color=red size=3px></font></b>